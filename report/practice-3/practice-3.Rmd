---
title: "report"
output: html_document
---

```{r, message=FALSE, warning=FALSE, echo=FALSE}
source(here::here("scripts/setup.R"))

library(reshape2)
library(copula)
```


```{r}
sales_10_adj <-
  read.csv(here::here("data/sales_10_adjusted.csv"))
```


## a)

The main challenge under this configuration is dealing the with the correlation between the products of the basket. As the VaR is not additive, figures of components of a basket do not add to the risk of the overall basket. This is because this measure does not take correlations into account and a simple addition could lead to double counting and underestimating the risk, especially when the tail dependence is rather high.

## b)

```{r chibar, echo=FALSE}
#CHI BAR
nb_prod <- length(sales_10_adj)

chi_bar <- matrix(NA, nb_prod, nb_prod)

colnames(chi_bar) <- 1:10
rownames(chi_bar) <- 1:10

for (i in 1:(nb_prod - 1)){
  for (j in (i + 1):nb_prod){
    chi_bar[i,j] <- extRemes::taildep(sales_10_adj[,i], sales_10_adj[,j], 0.9)[[2]]
  }
}

longChi_bar<-melt(chi_bar)

longChi_bar %>%
  filter(!is.na(value)) %>%
  ggplot(aes(x = factor(Var1), y = factor(Var2))) +
  geom_raster(aes(fill = value)) +
  scale_fill_gradient(low = "grey90", high = "blue") +
  labs(
    x = "product U",
    y = "product V",
    title = "Tail dependence for each pair of products",
    subtitle = "90% Quantile",
    fill = unname(latex2exp::TeX("$\\bar{\\chi}$"))
  )
```

The tail dependence can be measured thanks to the $\bar{\chi}$ metric and is given by the following equation : $$\bar{\chi}(u) = 2*log(Pr[U > u])/log(Pr[U > u, V > u]) - 1$$

Figure \@ref(fig:chibar) displays this mesure of dependence for each pair of products at the 90% quantile. We may easily notice that certain pairs of products are highly tail dependent.


```{r topchibar, echo=FALSE}
longChi_bar %>%
  filter(!is.na(value)) %>%
  arrange(desc(value)) %>%
  top_n(5, value) %>%
  unite("pair", Var1:Var2,sep = "-") %>%
  kable_maker(caption = "",
              col.names = c("Product pair",
                            "$\\bar{\\chi}$"))
```

Table \@ref(tab:topchibar) shows the top five pairs of product in terms of tail dependence.


```{r pavalchi, echo=FALSE}
nb_prod <- length(sales_10_adj)

pvalue <- matrix(NA, nb_prod, nb_prod)

for (i in 1:(nb_prod - 1)) {
  for (j in (i + 1):nb_prod) {
    pvalue[i, j] <-
      extRemes::taildep.test(sales_10_adj[, i], 
                             sales_10_adj[, j], 
                             cthresh =
                               -0.1)$p.value[[1]]
  }
}

longPval<-melt(pvalue)

longPval %>%
  filter(!is.na(value)) %>%
  arrange(desc(value)) %>%
  mutate(pvalue = case_when(value >= 0.05 ~ "> 5%",
                            value < 0.05 ~ "< 5%")) %>%
  ggplot(aes(x = factor(Var1), y = factor(Var2))) +
  geom_tile(aes(fill = pvalue), linetype = 1, colour = "grey50") +
  labs(
    x = "product U",
    y = "product V",
    title = "Tail dependence test for each pair of products",
    fill = "P-Value",
    subtitle = "90% Quantile"
  )
```

We may also apply a tail dependence test for each pair of products, the null hypothesis being the tail dependence.
Figure \@ref(fig:pavalchi) shows that a majority of pair of products may be considered tail dependent since their p-value is superior to 5% for a probability thresold of 90%.

```{r chiplothigh, echo=FALSE}
chiplot(cbind(sales_10_adj[,2], sales_10_adj[,3]), which = 2, main2 = "Chi Bar plot for product 2 & 3")
```

Chi Bar plot may also be usefull to detect tail dependency. As shown on Graph \@ref(fig:chiplothigh) , products 2 & 3 look strongly dependent in the upper tail.

```{r chiplotlow,echo=FALSE}
chiplot(cbind(sales_10_adj[,2], sales_10_adj[,10]), which = 2, main2 = "Chi Bar Plot for products 2 & 10")
```

Conversly, Figure \@ref(fig:chiplotlow) shows low tail-dependence for product 2 & 10.

## c)

The Gaussian copula is asymptotically independent in both upper and lower tails.
Since a majority of pairs are dependent in upper tail, the Gaussian copula seems inappropriate as it would lead to an under estimation of the VaR.

## d)
### .i

Prior to fitting the Copula, we apply a uniform transformation to the data. A gdp is fitted on data points above a thresold corresponding to the 90% quantile for each products. The instances below this quantile are treated in a different manner and are uniformised using their empircal cumulative distribution.

```{r echo=FALSE}
# Thresold : quantile @ 0.9s
thresholds <- numeric(10)

for (i in 1:10) {
  thresholds[i] <- quantile(sales_10_adj[,i], 0.9)
}

# Model for the tail : POT
fit_pot <- list()

for (i in 1:10) {
  fit_pot[[i]] <- evd::fpot(sales_10_adj[,i], thresholds[i])
}
```

```{r echo=FALSE}
transform_to_uniform <- function(y,
                                 cdf,
                                 threshold,
                                 scale,
                                 shape) {
  
  above <- y > threshold
  # proportion of exceedances
  p <- mean(above)
  # under the threshold, use the empirical CDF
  ecdf_below <- ecdf(y[!above])
  empirical <- ecdf_below(y[!above])
  # above, apply dgpd
  theoretical <- cdf(y[above],
    loc = threshold,
    scale = scale,
    shape = shape
  )
  
  transformed <- numeric(length = length(y))
  # "Glue together" the empirical and theoretical parts by rescaling them:
  
  #        empirical          e.g. N(0,1)
  # [-----------------------|-------------]
  # 0                       p             1
  
  transformed[!above] <- (1 - p) * empirical    # (1)
  transformed[above] <- 1 - p + p * theoretical # (2)
  
  return(list(
    transformed = transformed,
    ecdf = ecdf_below,
    prop = p
  ))
}
```


```{r echo=FALSE}
uniformised <- lapply(1:10, function (i) {
transform_to_uniform(sales_10_adj[,i], 
                     cdf = evd::dgpd, 
                     threshold = thresholds[i], 
                     scale = fit_pot[[i]]$estimate[1],
                     shape = fit_pot[[i]]$estimate[2])
  
})

# merging the u's
transformed <-  as.data.frame(uniformised[[1]]$transformed)
colnames(transformed) <- "product_1"

for (i in 2:10){
  temp <- as.data.frame(uniformised[[i]]$transformed)
  colnames(temp) <- paste0("product_",i)
  transformed <- cbind(transformed, temp)
}
```

### .ii 

```{r warning=FALSE, message=FALSE, echo=FALSE}
fit_results <- expand.grid(Copula = c("Gauss", "Gumbel", "Clayton", "T"),
                       param1 = NA,
                       param2 = NA,
                       LogLikelihood = NA,
                       AIC = NA)
```

```{r warning=FALSE, message=FALSE, echo=FALSE}
fit_normal <- fitCopula(normalCopula(dim = 10), transformed, method = "ml")


fit_results$param1[1] <-coef(fit_normal)[1]
fit_results$param2[1] <-"-"
fit_results$LogLikelihood[1] <- stats::logLik(fit_normal)
fit_results$AIC[1] <- stats::AIC(fit_normal)
```

```{r warning=FALSE, message=FALSE, echo=FALSE}
fit_gumbel <- fitCopula(gumbelCopula(dim = 10), transformed, method = "ml")

fit_results$param1[2] <-coef(fit_gumbel)[1]
fit_results$param2[2] <-"-"
fit_results$LogLikelihood[2] <- stats::logLik(fit_gumbel)
fit_results$AIC[2] <- stats::AIC(fit_gumbel)
```

```{r warning=FALSE, message=FALSE, echo=FALSE}
fit_clayton <- fitCopula(claytonCopula(dim = 10), transformed, method = "ml")

fit_results$param1[3] <-coef(fit_clayton)[1]
fit_results$param2[3] <-"-"
fit_results$LogLikelihood[3] <- stats::logLik(fit_clayton)
fit_results$AIC[3] <- stats::AIC(fit_clayton)
```

```{r warning=FALSE, message=FALSE, echo=FALSE}
fit_t <- fitCopula(tCopula(dim = 10), transformed)

rho <- coef(fit_t)[1]
df <- coef(fit_t)[2]

fit_results$param1[4] <-coef(fit_t)[1]
fit_results$param2[4] <-round(coef(fit_t)[2],3)
fit_results$LogLikelihood[4] <- stats::logLik(fit_t)
fit_results$AIC[4] <- stats::AIC(fit_t)
```


```{r resultscopu}
fit_results %>% arrange(AIC) %>%
  kable_maker(caption="Employer rating: Comparison with 2 sentiment analysis methods",
                                      col.names=c("Copula",
                                                  "Parameter 1",
                                                  "Parameter 2",
                                                  "Log-Likelihood",
                                                  "AIC"
                                                  ))
```

Table \@ref(tab:resultscopu) displays the Coppulas that have been fitted to the transformed data. With an AIC of -508, the T coppula suits the best the data and will be retained for the following of our analysis.

### .iii

After selecting the best coppula, we generate 1913 instances for each products to which we apply the inverse of the transformation that has been made in point i.

```{r echo=FALSE}
set.seed(1234)
simulated <- rCopula(1913, tCopula(param = rho, dim = 10, df = df))
# simulated <- rCopula(1913, normalCopula(fit_results$param1[1], dim = 10))
# simulated <- rCopula(1913, gumbelCopula(fit_results$param1[2], dim = 10))
# simulated <- rCopula(1913, claytonCopula(fit_results$param1[3], dim = 10))
```

```{r echo=FALSE}
inverse_transform <- function(u, ecdf, quantile_function, p, threshold, scale, shape) {
  above <- u > 1 - p
  original_scale <- numeric(length = length(u))
  original_scale[!above] <- quantile(ecdf, u[!above] / (1 - p))
  original_scale[above] <-
    quantile_function((u[above] - (1 - p)) / p, threshold, scale, shape)
  return(original_scale)
}
```

```{r echo=FALSE}
simulated_inversed <- sapply(1:10, function (i) {
inverse_transform(u = simulated[,i], 
                  ecdf = uniformised[[i]]$ecdf,
                  quantile_function = evd::qgpd,
                  p = uniformised[[i]]$prop,
                  threshold = thresholds[i],
                  scale = fit_pot[[i]]$estimate[1],
                  shape = fit_pot[[i]]$estimate[2])
  
})
```


### .iv

```{r echo=FALSE}
#Generated + retransformed
S <- rowSums(simulated_inversed)
VaR <- quantile(S, 0.95)
```

The Value at Risk for the simulated data is obtained by taking the 95 % quantile of the row sums and corresponds to `r VaR`

<!-- ```{r} -->
<!-- #Original Data -->
<!-- quantile(rowSums(sales_10_adj), 0.95) -->
<!-- ``` -->

<!-- Sum of the VaR -->
<!-- ```{r} -->
<!-- #Generated + retransformed -->
<!-- sum(apply(simulated_inversed, 2, quantile, 0.95)) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- # original data -->
<!-- sum(apply(sales_10_adj, 2, quantile, 0.95)) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- fit_simulated <- evd::fpot(S, quantile(S, 0.9)) -->


<!-- u <- fit_simulated$threshold -->
<!-- p <- length(fit_simulated$exceedances)/length(S) -->


<!-- VAR <- u + fit_simulated$estimate[1]/fit_simulated$estimate[2] * (((1-0.95)/p)^-fit_simulated$estimate[2]-1) -->

<!-- VAR[[1]] -->
<!-- ``` -->


<!-- ```{r message=FALSE, warning=FALSE} -->

<!-- VaR <- numeric(10) -->

<!-- for (i in 1:10) { -->
<!-- mod <- QRM::fit.GPD(simulated_inversed[,i], threshold = quantile(simulated_inversed[,i], 0.9)) -->
<!-- VaR[i] <- QRM::RiskMeasures(mod, c(0.95))[2] -->
<!-- } -->

<!-- sum(VaR) -->
<!-- ``` -->

