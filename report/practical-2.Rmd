# Practical 2: Demand monitoring, part I

```{r, echo = FALSE, message = FALSE, include=FALSE}
source(here::here("scripts/setup.R"))
```

## part a - out of stock products

```{r}
sales_data <-
  read.delim(here::here("data/sales_1.txt"), col.names = "Sales") %>% mutate(Day = row_number())
```

```{r}
stockout <- which(diff(sales_data$Sales) == -sales_data$Sales & sales_data$Sales !=0)

ggplot(data = sales_data, aes(x = Day, y = Sales)) +
  geom_line()+
  geom_point()+
  geom_vline(xintercept =stockout, color= "red") +
  labs(title = "Sales unit by day")+
  theme(plot.title = element_text(hjust = 0.5))
```

The red line shows when the observed number of sales goes to zero, essentially the extreme of the sales, there are also other instances where it nears to zero however we ignore them for simplicity sake. We run of stock in these cases.

From the graph above, we can also notice that right before going out of stock, the sales are extremely high. When looking at the first block, from day 0 to day 330 which is the first stockout, the sales data are distributes as follows:

```{r}
# sales_data %>% 
#   arrange(Sales) %>%
#   head() %>%
#   kable_maker(caption="The first 6 lowest sales")

sales_data %>% 
  filter(Day < 330) %>% 
  ggplot(aes(x=Day, y = Sales))+
  geom_point() +
  labs(title = "Sales unit in the first 330 days")+
  theme(plot.title = element_text(hjust = 0.5))
```

One can clearly notice that from the first few observations showed, there is a positive trend in the sales, which the vendors were not able to forecast hence the stockout at Day 330. 

We can now look at what happened between the first and the second stockout.
```{r}
sales_data %>% 
  filter(Day > 330 & Day < 383) %>% 
  ggplot(aes(x=Day, y = Sales))+
  geom_point() +
  labs(title = "Sales unit in the between the 1st and the 2nd stockout")+
  theme(plot.title = element_text(hjust = 0.5))
```
One can notice that following the first stockout there isn't a particular structure in the sales data, it looks like the sales are random. Moreover, the stockout lasted 2 days. Hence, we can say that the second stockout is probably due to mismanagement following the initial positive trend of the data. 

We can now observe what the sales data between day second and thrid time they went out of stock:

```{r}
sales_data %>% 
  filter(Day > 394 & Day < 696) %>% 
  ggplot(aes(x=Day, y = Sales))+
  geom_line() +
  labs(title = "Sales unit in the between the 2nd and the 3rd stockout")+
  theme(plot.title = element_text(hjust = 0.5))
```
Looking at the sales data distribution in the above graph, one can notice that on the one hand the Sales demand until Day 600 seems kind of linear, but on the other hand following this day it looks like there is once again a positive trend. 

Going over each block of data is not necessary as the observations above are already conclusive: the vendors go out of stock when there is an increasing demand. The sellers were probably not able to forecast the positive trends and to manage the inventory given the increasing sales, leading them to stockout.


## part b - newsvendor model
Newsvendor model.

Knowing the newsvendor model we can easily compute p:
```{r}
# Manual approach
newsvendor_mod <- function(price,cost,unit_salvage){
  salvage <- unit_salvage * cost
  underage <- price - cost
  overage <- cost - salvage
  fractile <- underage / (overage + underage)
  return(fractile)
}

cf_news <- newsvendor_mod(10,1,0.1)

cf_news
```

Alternatively, we could have used the [Scperf](https://cran.r-project.org/web/packages/SCperf/SCperf.pdf) library and the `Newsboy` function.


```{r}
# Using a library
library(SCperf)
mean_s <- mean(sales_data$Sales)
sd_s <- sd(sales_data$Sales)
Newsboy(m= mean_s,sd= sd_s,p=10,c=1,s=0.1)
```

Hence, the critical fractile will be computed as $$q = F^-1(`r cf_news`)$$. 

## part c - critical fractile and Value at Risk
Value-at-Risk (VaR) also known as chance constraint is used in the newsvendor model as a constraint, limiting the probability of particular event (in this case stock-out) happening. In fact, we can say that the critical fractile of the newsvendor model is the equivalent the level of the Value-at-Risk  (`r cf_news`): we don't want to go beyond this value to avoid entering the extreme values. 

Expected shortfall or conditional or sometimes called conditional value-at-risk , is about 'how bad can things get?'. More concrectly, one could compute the Value-at-Risk at a level of  `r cf_news`, and compute the expected shortfall which indicates the average value of sales when we enter the extremes of the distribution, hence when the sales are extremely high which, as we saw in the beginning of this analysis, is likely to lead to a stockout.

## part d - model using the poisson distribution

```{r}
# Manual approach
newsvendor_mod <- function(price,cost,unit_salvage){
  salvage <- unit_salvage * cost
  underage <- price - cost
  overage <- cost - salvage
  fractile <- underage / (overage + underage)
  return(fractile)
}

cf_news <- newsvendor_mod(10,1,0.1)

newsvendor_mod
```

Using the proposed function, we can fit the Poisson model to our data an compute the estimated μ by using the MLE.

```{r, warning=FALSE}
sales_poisson <- fitdistr(sales_data$Sales, "Poisson")
mu <- sales_poisson$estimate
cf_pois <- qpois(cf_news,sales_poisson$estimate)
```
After having fitted a Poisson to our data, we obtain μ = `r mu`. This allows us to compute the critical fractile: according to this model, the estimated critical fractile is `r cf_pois`.

## part e - peaks-over-thresholds model
We are now going to compute a POT model. First we have to define the threshold.

```{r,message = FALSE,warning=FALSE}
u_plot <- function (a_column) {
  min_col <- min(a_column)
  max_col <- max(a_column)
  mrlplot(a_column,
          umin = min_col,
          umax = max_col)
  upper_threshold <- (max_col-(max_col*0.1))
  tcplot(a_column,
         tlim = c(min_col, upper_threshold),
         std.err = FALSE)
}

u_plot(sales_data$Sales)
```

Thresholds of 150 and 170 seem interesting, we can test for both. 

```{r}
# different_u <- list(u150=(150, u170= 170)
# different_u <- list(u150=(150, u170= 170)
u150 <- 150
u170 <- 170
# Later have to use a mutate+ across + functions in order no to have code duplicates 
sales_data$col1 <- cut(sales_data$Sales,
               breaks = c(-Inf, u150, Inf),
               labels = c("<=u", ">u")) 
sales_data$col2 <- cut(sales_data$Sales,
               breaks = c(-Inf, u170, Inf),
               labels = c("<=u", ">u"))

# we create a function to make our plots
plotter <- function (a_column, u_threshold) {
  sales_data %>%
    ggplot() +
    geom_line(aes(Day, Sales)) +
    geom_vline(xintercept = stockout, color = "blue") +
    geom_point(aes(Day, Sales, color = get(a_column))) +
    scale_colour_manual(values = c('black', 'red')) +
    geom_hline(yintercept = u_threshold,
               linetype = 2,
               colour = "red") +
    guides(color = FALSE) +
    labs (
      title = paste(
        "Sales above the threshold of",
        u_threshold,
        "are highlighted in red"
      ),
      subtitle = "Blue lines represent day when the sales are 0"
    )
}

plotter("col1",u150)
plotter("col2", u170)
```


```{r}
gpd_u150 <- fpot(sales_data$Sales, threshold = u150, model = c("gpd"))
gpd_u170 <- fpot(sales_data$Sales, threshold = u170, model = c("gpd"))

par(mfrow = c(1,2))
plot(gpd_u150, which = 1)
plot(gpd_u170, which = 1)
```

Comparing the pp-plots of the two models, we decide to use the threshold of 150 as it provides better results. Thus, we will use the model with this threshold to compute the Value at Risk. We will obtain the following Value at Risk:

```{r}
# computing sales over the threshold
n.sales <- sales_data %>%
  filter(Sales > u150) %>%
  summarize(n = n()) %>%
  pull(n)

# f-bar
p.sales <- n.sales / length(sales_data$Sales)

# Var formula
VAR <-
  u150 + gpd_u150$estimate[1] / gpd_u150$estimate[2] * (((1 - cf_news) /
                                                           p.sales) ^ -gpd_u150$estimate[2] - 1)

VAR[[1]]
```

## part f - binomial backtesting

First we create the test set that we're going to use as well as setting
```{r}
# We create a test set that we can use
sales_test <- sales_data %>% tail(300)

# Then we set the train ize
train_size <- 365

# This will be use later for indexing during the moving window 
n_above <- nrow(sales_data)+1
```

### Poisson model
```{r}
# We initiate an empty vector to store our results for poisson
risk_values <- seq_along(sales_test$Sales)

for (i in seq_along(sales_test$Sales)) {
  # chosen window
  window <- sales_data %>% slice((n_above - i - train_size):(n_above - i))
  # we fit the distribution
  sales_poisson <- MASS::fitdistr(window$Sales, "Poisson")
  # calculating the value at risk
  risk_values[i] <- qpois(cf_news,sales_poisson$estimate)
  # Bernoulli violations
  risk_values[i] <- ifelse(risk_values[i] > sales_test$Sales[i], 0, 1)
}

# theoretical violations
sales_theory <-nrow(sales_test) * (1-cf_news)

# summing up violations
sales_violations <- sum(risk_values)

binom.test(sales_violations,nrow(sales_test), p=1-cf_news)
```
Looking at the binomial test we see that the p-value is really low. Thus, we have to reject the null hypothesis, meaning that the observed number of violations is not equal to the expected number of violations. 


### POT model
```{r}
# Create a function for our models
sales_risk <- function(u_threshold, a_dataframe, a_column, alpha) {
  # filtering for observations above a threshold
  sales_above <-
    a_dataframe %>% filter({
      {
        a_column
      }
    } > u_threshold) %>% pull({
      {
        a_column
      }
    })
  # fitting the gpd
  sales_gpd <- sales_above %>%
    fpot(., threshold = u_threshold, model = c("gpd"))
  
  # This creates problems and has to be resolved
  fbar <- length(sales_above) / nrow(sales_test)
  
  # we can extract the the shape, scale and use the alpha that we would like
  u_threshold + (sales_gpd$estimate[["scale"]] / sales_gpd$estimate[["shape"]]) *
    (((1 - alpha) / fbar) ^ -sales_gpd$estimate[["shape"]] - 1)
  }

# applying it as an example
# sales_risk(150, sales_data, Sales, cf_news)

# loop over for calculating the VAR of 365 days
risk_values <- seq_along(sales_test$Sales)

for (i in seq_along(sales_test$Sales)) {
  # chosen window
  window <- sales_data %>% slice((n_above - i - train_size):(n_above - i))
  # calculating the value at risk
  risk_values[i] <- sales_risk(150, window, Sales, cf_news)
  # Bernoulli violations
  risk_values[i] <- ifelse(risk_values[i] > sales_test$Sales[i], 0, 1)
}

# theoretical violations
sales_theory <-nrow(sales_test) * (1-cf_news)

# summing up violations
sales_violations <- sum(risk_values)

binom.test(sales_violations,nrow(sales_test), p=1-cf_news) 
```

At a level of 5%, we are once again going to reject the null hyopthesis that te observed number of violations is the same as the estimated one. However, we draw a different conclusion at a level of 1%. In this case when looking at the p-value, one can observe that the null hypothesis can't be rejected.