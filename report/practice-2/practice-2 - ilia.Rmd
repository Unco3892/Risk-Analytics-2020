---
title: "report"
output: html_document
---

```{r, echo = FALSE, message = FALSE, include=FALSE}
source(here::here("scripts/setup.R"))
```

# a)

```{r}
sales_data <-
  read.delim(here::here("data/sales_1.txt"), col.names = "Sales") %>% mutate(Day = row_number())
```

```{r}
stockout <- which(diff(sales_data$Sales) == -sales_data$Sales & sales_data$Sales !=0)

ggplot(data = sales_data, aes(x = Day, y = Sales)) +
  geom_line()+
  geom_point()+
  geom_vline(xintercept =stockout, color= "red") +
  labs(title = "Sales unit by day")+
  theme(plot.title = element_text(hjust = 0.5))
```
The red line shows when the observed number of sales goes to zero, essentially the extreme of the sales, there are also other instances where it nears to zero however we ignore them for simplicity sake. We run of stock in these cases.

From the graph above, we can also notice that right before going out of stock, the sales are extremely high. When looking at the first block, from day 0 to day 330 which is the first stockout, the sales data are distributes as follows:
```{r}
sales_data %>% 
  arrange(Sales)

sales_data %>% 
  filter(Day < 330) %>% 
  ggplot(aes(x=Day, y = Sales))+
  geom_point() +
  labs(title = "Sales unit in the first 330 days")+
  theme(plot.title = element_text(hjust = 0.5))

```
One can clearly notice that there is a positive tren in the sales, which the vendors were not able to forecast hence the stockout at Day 330. 

We can now look at what happened between the first and the second stockout.
```{r}
sales_data %>% 
  arrange(Sales)

sales_data %>% 
  filter(Day > 330 & Day < 383) %>% 
  ggplot(aes(x=Day, y = Sales))+
  geom_point() +
  labs(title = "Sales unit in the between the 1st and the 2nd stockout")+
  theme(plot.title = element_text(hjust = 0.5))

```
One can notice that following the first stockout there isn't a particular structure in the sales data, it looks like the sales are random. Moreover, the stockout lasted 2 days. Hence, we can say that the second stockout is probably due to mismanagement following the initial positive trend of the data. 

We can now observe what the sales data between day second and thrid time they went out of stock:

```{r}
sales_data %>% 
  arrange(Sales)

sales_data %>% 
  filter(Day > 394 & Day < 696) %>% 
  ggplot(aes(x=Day, y = Sales))+
  geom_line() +
  labs(title = "Sales unit in the between the 2nd and the 3rd stockout")+
  theme(plot.title = element_text(hjust = 0.5))

```
Looking at the sales data distribution in the above graph, one can notice that n the one hand the Sales demand until Day 600 seems kind of linear, but on the other hand following this day it looks like there is once again a positive trend. 

Going over each block of data is not necessary as the observations above are already conclusive: the vendors go out of stock when there is an increasing demand. The sellers were probably not able to forecast the positive trends and to manage the inventory given the increasing sales, leading them to stockout.


# b)
Newsvendor model.

Knowing the newsvendor model we can easily compute p:
```{r}
# Manual approach
newsvendor_mod <- function(price,cost,unit_salvage){
  salvage <- unit_salvage * cost
  underage <- price - cost
  overage <- cost - salvage
  fractile <- underage / (overage + underage)
  return(fractile)
}

cf_news <- newsvendor_mod(10,1,0.1)

cf_news
```

Alternatively, we could have used the [Scperf](https://cran.r-project.org/web/packages/SCperf/SCperf.pdf) library and the `Newsboy` function.


```{r}
# Using a library
library(SCperf)
mean_s <- mean(sales_data$Sales)
sd_s <- sd(sales_data$Sales)
Newsboy(m= mean_s,sd= sd_s,p=10,c=1,s=0.1)
```

Hence, the critical fractile will be computed as q = F^-1(`r cf_news`). 
# PIA: is it possible to write the equation in a nicer way?

# c)
Value-at-Risk (VaR) also known as chance constraint is used in the newsvendor model as a constraint, limiting the probability of particular event (in this case stock-out) happening. In fact, we can say that the critical fractile of the newsvendor model is the equivalent to Value-at-Risk with level `r cf_news`: we don't want to go beyond this value to avoid entering the extreme values. 

VaR answers the question of how much one can lose with
x% probability over a given time horizon.

In this case, we have much more underage cost than overage giving us a ratio of `r cf_news`.

Expected shortfall or conditional or sometimes called conditional value-at-risk , is about 'how bad can things get?'. More concrectly, one could compute the Value-at-Risk at a level of  `r cf_news`, and compute the expected shortfall which indicates the average value of sales when we enter the extremes of the distribution, hence when the sales are extremely high which, as we saw in the beginning of this analysis, is likely to lead to a stockout.

# d)
Poisson model.

using maximum likelihood estimation for μ.

```{r}
# Manual approach
newsvendor_mod <- function(price,cost,unit_salvage){
  salvage <- unit_salvage * cost
  underage <- price - cost
  overage <- cost - salvage
  fractile <- underage / (overage + underage)
  return(fractile)
}

cf_news <- newsvendor_mod(10,1,0.1)

cf_news
```

Using the proposed function, we can fit the Poisson model to our data an compute the estimated μ by using the MLE.

```{r, warning=FALSE}
library (MASS)
sales_poisson <- MASS::fitdistr(sales_data$Sales, "Poisson")
mu <- sales_poisson$estimate
critical_fractile_poisson <- qpois(cf_news,sales_poisson$estimate)
```
After having fitted a Poisson to our data, we obtain μ = `r mu`. This allows us to compute the critical fractile: according to this model, the estimated critical fractile is `r critical_fractile_poisson`.

# e)

POT model.

First we have to determine a value for `u`.

```{r,message = FALSE,warning=FALSE}
u_plot <- function (a_column) {
  min_col <- min(a_column)
  max_col <- max(a_column)
  mrlplot(a_column,
          umin = min_col,
          umax = max_col)
  upper_threshold <- (max_col-(max_col*0.1))
  tcplot(a_column,
         tlim = c(min_col, upper_threshold),
         std.err = FALSE)
}

u_plot(sales_data$Sales)

```

Thresholds of 150 and 170 seem interesting, we can test for both.

```{r}
# different_u <- list(u150=(150, u170= 170)

u150 <- 150
u170 <- 170

# Later have to use a mutate+ across + functions in order no to have code duplicates 
sales_data$col1 <- cut(sales_data$Sales,
               breaks = c(-Inf, u150, Inf),
               labels = c("<=u", ">u")) 

sales_data$col2 <- cut(sales_data$Sales,
               breaks = c(-Inf, u170, Inf),
               labels = c("<=u", ">u"))

cutter <- function(a_column, u_threshold,a_name){
  sales_data$hi <- cut(sales_data$Sales,
               breaks = c(-Inf, u_threshold, Inf),
               labels = c("<=u", ">u"))
}

sales_data %>%
  ggplot() +
  geom_line(aes(Day, Sales)) +
  geom_vline(xintercept =stockout, color= "blue") +
  geom_point(aes(Day, Sales, color = col1)) +
  scale_colour_manual(values = c('black', 'red')) +
  geom_hline(yintercept = u150,
             linetype = 2,
             colour = "red") +
  guides(color = FALSE) +
  labs (title = "Sales above the threshold of 150 are highlighted in red",
        subtitle = "Blue lines represent day when the sales are 0")

cutter(sales_data$Sales, u170)

sales_data %>%
  ggplot() +
  geom_line(aes(Day, Sales)) +
  geom_vline(xintercept =stockout, color= "blue") +
  geom_point(aes(Day, Sales, color = col2)) +
  scale_colour_manual(values = c('black', 'red')) +
  geom_hline(yintercept = u170,
             linetype = 2,
             colour = "red") +
  guides(color = FALSE) + 
  labs (title = "Sales above the threshold of 170 are highlighted in red",
        subtitle = "Blue lines represent day when the sales are 0")

# The code below is to be removed later
# for (i in seq_along(different_u)){
#   cutter(sales_data$Sales, different_u[[i]])
#   
# }
# cutter(sales_data$Sales, u150)

```
In order to define which threshold is the best, we are going to fit a GPD to the difference between the extremes and the thresholds, then use the AIC criteria to define the best model.

```{r}
u120 <- 120
u190 <- 170


gpd_u150 <- fpot(sales_data$Sales, threshold = u150, model = c("gpd"))
gpd_u170 <- fpot(sales_data$Sales, threshold = u170, model = c("gpd"))

par(mfrow = c(2,2))
plot(gpd_u150, which = 1)
plot(gpd_u170, which = 1)

```

Comparing the pp-plots of the two models, we decide to use the threshold of 150 as it provides better results. Thus, we will use the model with this threshold to compute the Value at Risk.

```{r}
n.sales <- sales_data %>%
  filter(Sales > u150) %>%
  summarize(n = n()) %>%
  pull(n)

p.sales <- n.sales/length(sales_data$Sales)

VAR <- u150 + gpd_u150$estimate[1]/gpd_u150$estimate[2] * (((1-cf_news)/p.sales)^-gpd_u150$estimate[2]-1)

VAR[[1]]

```

# f)

Back test as seen on the slides. We will go over this in more detail.

## d
```{r}
sales_poisson <- MASS::fitdistr(sales_data$Sales, "Poisson")
mu <- sales_poisson$estimate
critical_fractile_poisson <- qpois(cf_news,sales_poisson$estimate)
```

```{r}
# We create a test set that we can use
sales_test <- sales_data %>% tail(300)

# Create a function for our models
sales_risk <- function(u_threshold, a_dataframe, a_column, alpha) {
  # filtering for observations above a threshold
  sales_above <-
    a_dataframe %>% filter({
      {
        a_column
      }
    } > u_threshold) %>% pull({
      {
        a_column
      }
    })
  # fitting the gpd
  sales_gpd <- sales_above %>%
    fpot(., threshold = u_threshold, model = c("gpd"))
  
  # This creates problems and has to be resolved
  fbar <- length(sales_above) / nrow(sales_test)
  
  # we can extract the the shape, scale and use the alpha that we would like
  u_threshold + (sales_gpd$estimate[["scale"]] / sales_gpd$estimate[["shape"]]) *
    (((1 - alpha) / fbar) ^ -sales_gpd$estimate[["shape"]] - 1)
  }

# applying it as an example
sales_risk(120, sales_data, Sales, cf_news)

# loop over for calculating the VAR of 365 days
train_size <- 365
risk_values <- seq_along(sales_test$Sales)
n_above <- nrow(sales_data)+1

for (i in seq_along(sales_test$Sales)) {
  # chosen window
  window <- sales_data %>% slice((n_above - i - train_size):(n_above - i))
  # calculating the value at risk
  risk_values[i] <- sales_risk(120, window, Sales, cf_news)
  # Bernoulli violations
  risk_values[i] <- ifelse(risk_values[i] > sales_test$Sales[i], 0, 1)
}

# theoretical violations
sales_theory <-nrow(sales_test) * (1-cf_news)

# summing up violations
sales_violations <- sum(risk_values)


```

We can also do use the 90th of the data for the threshold.
```{r}
train_size <- 365
risk_values <- seq_along(sales_test$Sales)
n_above <- nrow(sales_data)+1
threshold_record <- risk_values
  
for (i in seq_along(sales_test$Sales)) {
  # chosen window
  window <- sales_data %>% slice((n_above - i - train_size):(n_above - i))
  # calculating the value at risk
  threshold <- quantile(window$Sales, 0.9)
  # we record the threshold we have chosen
  threshold_record[i] <- threshold
  risk_values[i] <- sales_risk(threshold, window, Sales, cf_news)
  # Bernoulli violations
  risk_values[i] <- ifelse(risk_values[i] > sales_test$Sales[i], 0, 1)
}

# theoretical violations
sales_theory <-nrow(sales_test) * (1-cf_news)

# summing up violations
sales_violations <- sum(risk_values)


# Naturally our critical fractile is the alpha here, right?
prop.test((nrow(sales_test)-sales_violations),nrow(sales_test), cf_news)

prop.test(sales_violations,nrow(sales_test), p=1-cf_news)

# Or the method below
binom.test(sales_violations, nrow(sales_test), p=1-cf_news)

```

## f
d and e



